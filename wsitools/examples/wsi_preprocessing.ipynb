{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download example image\n",
    "We can download some example images from this site: https://openslide.cs.cmu.edu/download/openslide-testdata/\n",
    "Use the following code (or manually) to download the example image data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "('CMU-2.svs', <http.client.HTTPMessage at 0x7f5e70608820>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "download_url = \"https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-2.svs\"\n",
    "urllib.request.urlretrieve(download_url, \"CMU-2.svs\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T15:05:36.486622200Z",
     "start_time": "2024-10-07T15:04:53.582934900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract image patches from whole slide images\n",
    "Please refer to: https://github.com/smujiang/WSITools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs\n",
      "./patches\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "wsi_fn = \"./CMU-2.svs\"             # Define a sample image that can be read by OpenSlide\n",
    "output_dir = \"./patches\"    # Define an output directory\n",
    "log_dir = \"./logs\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(log_dir)\n",
    "print(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T15:41:47.609595300Z",
     "start_time": "2024-10-07T15:41:47.608676300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 10:42:08.796734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-07 10:42:09.890536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from wsitools.tissue_detection.tissue_detector import TissueDetector\n",
    "from wsitools.patch_extraction.patch_extractor import ExtractorParameters, PatchExtractor\n",
    "\n",
    "# Define the parameters for Patch Extraction, including generating an thumbnail from which to traverse over to find\n",
    "# tissue.\n",
    "parameters = ExtractorParameters(output_dir, # Where the patches should be extracted to\n",
    "    save_format = '.png',                      # Can be '.jpg', '.png', or '.tfrecord'\n",
    "    sample_cnt = -1,                           # Limit the number of patches to extract (-1 == all patches)\n",
    "    patch_size = 128,                          # Size of patches to extract (Height & Width)\n",
    "    rescale_rate = 128,                        # Fold size to scale the thumbnail to (for faster processing)\n",
    "    patch_filter_by_area = 0.5,                # Amount of tissue that should be present in a patch\n",
    "    with_anno = True,                          # If true, you need to supply an additional XML file\n",
    "    extract_layer = 0,                          # OpenSlide Level\n",
    "    log_dir=log_dir\n",
    "    )\n",
    "\n",
    "# Choose a method for detecting tissue in thumbnail image\n",
    "tissue_detector = TissueDetector(\"LAB_Threshold\",   # Can be LAB_Threshold or GNB\n",
    "    threshold = 85,                                   # Number from 1-255, anything less than this number means there is tissue\n",
    "    training_files = None                             # Training file for GNB-based detection\n",
    "    )\n",
    "\n",
    "# Create the extractor object\n",
    "patch_extractor = PatchExtractor(tissue_detector,\n",
    "    parameters,\n",
    "    feature_map = None,                       # See note below\n",
    "    annotations = None                        # Object of Annotation Class (see other note below)\n",
    "    )\n",
    "\n",
    "patch_extractor.extract([wsi_fn])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-07T15:42:07.600124200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show where the patches were extracted from"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "log_img_fn = os.listdir(log_dir)[0]\n",
    "log_img = os.path.join(log_dir, log_img_fn)\n",
    "img = Image.open(log_img)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalize the extracted patches\n",
    "Need to install staintools. Please refer to https://github.com/Peter554/StainTools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import staintools\n",
    "from PIL import Image\n",
    "template_img = \"./template.png\"  # TODO: select an template image, should be the same size of the image tiles to be normalized\n",
    "normalized_patches_dir = \"./normalized_patches\"\n",
    "if not os.path.exists(normalized_patches_dir):\n",
    "    os.makedirs(normalized_patches_dir)\n",
    "\n",
    "img_fn_list = os.listdir(output_dir)\n",
    "for img_fn in img_fn_list:\n",
    "    target = staintools.read_image(os.path.join(output_dir, img_fn))\n",
    "    to_transform = staintools.read_image(template_img)\n",
    "\n",
    "    # Standardize brightness (optional, can improve the tissue mask calculation)\n",
    "    target = staintools.LuminosityStandardizer.standardize(target)\n",
    "    to_transform = staintools.LuminosityStandardizer.standardize(to_transform)\n",
    "\n",
    "    # Stain normalize\n",
    "    normalizer = staintools.StainNormalizer(method='vahadane')\n",
    "    normalizer.fit(target)\n",
    "    transformed = normalizer.transform(to_transform)\n",
    "\n",
    "    sv_fn = os.path.join(normalized_patches_dir, img_fn)\n",
    "    transformed.save(sv_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download pretrained CTransPath model\n",
    "Please refer to this site: https://github.com/Xiyue-Wang/TransPath.\n",
    "The model can be downloaded from here: https://drive.google.com/file/d/1DoDx_70_TLj98gTf6YTXnu4tFhsFocDX/view?usp=sharing\n",
    "Download the file, and save to \"./CTransPath/ctranspath.pth\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_sv = \"./CTransPath/ctranspath.pth\""
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get image embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from ctran import ctranspath\n",
    "\n",
    "class roi_dataset(Dataset):\n",
    "    def __init__(self, img_csv,):\n",
    "        super().__init__()\n",
    "        self.transform = trnsfrms_val\n",
    "        self.images_lst = img_csv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_lst)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.images_lst.filename[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "embeddings_csv = \"./embeddings/img_embeddings.csv\"\n",
    "if not os.path.exists(os.path.split(embeddings_csv)[0]):\n",
    "    os.makedirs(os.path.split(embeddings_csv)[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    mean = (0.6373, 0.5260, 0.7438)\n",
    "    std = (0.1089, 0.1249, 0.0710)\n",
    "    trnsfrms_val = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    img_csv = pd.read_csv(r'All_HE_img_samples.csv')\n",
    "    test_datat = roi_dataset(img_csv)\n",
    "    database_loader = torch.utils.data.DataLoader(test_datat, batch_size=10, shuffle=False)\n",
    "\n",
    "    model = ctranspath()\n",
    "    model.head = nn.Identity()\n",
    "    td = torch.load(model_sv)\n",
    "    model.load_state_dict(td['model'], strict=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    embed_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in database_loader:\n",
    "            features = model(batch)\n",
    "            features = features.cpu().numpy()\n",
    "\n",
    "            embed_list.append(features)\n",
    "\n",
    "    all_embeds = np.concatenate(embed_list)\n",
    "    ##embeddings_standardized = StandardScaler().fit_transform(all_embeds)\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "    np.savetxt(embeddings_csv, all_embeds, delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Result visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(embeddings_csv).astype(float)\n",
    "img_features = np.array(df.iloc[:, :-1])\n",
    "lb = list(df.iloc[:, -1])\n",
    "lb_int = [int(x) for x in lb]\n",
    "\n",
    "dm_red = umap.UMAP(random_state=12)\n",
    "pca_cell_f = dm_red.fit_transform(img_features)\n",
    "\n",
    "plt.scatter(pca_cell_f[:, 0], pca_cell_f[:, 1], marker=\".\", s=1)\n",
    "plt.title(\"Image embeddings UMap\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
